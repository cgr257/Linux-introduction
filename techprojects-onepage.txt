Welcome to TechProjects!
This site is for people with a do-it-yourself attitude and some extra time and hardware. If you have an old computer laying around, don't let it go to waste! There are plenty of projects you can do to re-purpose old hardware that are simultaneously, fun, educational and practical.
Learning a little bit about how computers work will take you a long way. We live in an increasingly technology reliant world, and I find it disappointing that so many people spend so much time using devices that they don't fully understand. So often we just want the newest and best tech toys, but we frequently fail to recognize that computers are tools, and when we use them, we generally have a specific purpose in mind - it is always a good idea to have a better understanding of how to accomplish your goals, and goals that we aim to achieve through the use of technology are no exception.
If you have ever been curious about how something works; if you've ever wondered what lies at the heart of the technology you use every day; if you've ever thought to yourself: "I bet I could make something that does that!" then you are the kind of person that needs a project! Look around, see if you can find some ideas for a place to start your own project. Keep in mind, I made this site for a school assignment, so it isn't the best source of information if you have a genuine desire to learn (I tried to include links to relevant resources where possible) but what I have posted here comes from my own unique experience with technology based projects, and it is therefore new information that someone could potentially benefit from. Hopefully you can find something here that leads you down a road you might not have otherwise traveled.
Alright! let's get started on some new projects! 
Projects
There are several categories of projects on this site. All are things that I have spent some time learning, but I am no expert in any of it. This is meant to be a way to get some ideas of things to do on your own. I encourage you to look things up on your own, experiment, and do whatever works best for you, but above all: RTFM and don't do anything to your computer or run any code if you don't know what it'll do. You are responsible for your own actions.

Linux Projects
Many of the projects here make use of Free/Libre/Open Source Software (FLOSS). Being able to use a GNU/Linux distribution makes these sorts of projects much easier. A big reason that it's good to use GNU/Linux is becuause it's free in two ways- free as in "free speech" (libre) and free as in "free beer" (gratis). Both of these things are good for a do-it-yourselfer. If you're looking into starting a project that you're not very confident about, it would be difficult to put down a few hundred dollars on software - that's why it's great to have an operating system that comes at no monetary cost. If you want to do something that contributes to your own personal or professional growth, I think its best to stick to something that was created with the free sharing of information in mind - that's why it's great to have an operating system that comes with no heavy handed licensing requirements and that allows you to modify it at will.

LAMP projects
In this section we will look at very basic uses for a common server solution stack that uses Linux, Apache, MySQL, Python and PHP (LAMP servers).

Networking Projects
Most people have a home network (LAN), but few people use it to its full potential. Here we'll discuss several projects you can do on your own network that will help you to realize the true potential of your networked computers.
Linux Projects

 
If you've never used linux before, and you'd like to try it, you have a few options. The first is to install it on an old computer that you don't mind tinkering with. Another option is to install it to a virtual machine like virtualbox so that you don't have to acually run the risk of reinstalling the OS on any of your real hardware.
Once you have decided where you would like to install Linux, you have to decide which distribution to go with. Unlike Microsoft Windows and Mac OS, Linux is open source, meaning that anyone with the skill and desire can modify, add to, personalize, and redistribute it. Because of this, there are lots of different flavors of Linux called "Distributions" or "Distros." You can find a pretty comprehensive list of the most popular Linux Distros at Distrowatch.com.
For someone who has never used Linux before, I would recommend something like Linux Mint or Ubuntu

Linux Project #1 - Basic Bash Commands
This project is an introduction to working with the command line interface (CLI) in linux. The command line is the most basic way of telling the computer what to do (issuing commands). Being comfortable on the command line is a must for anyone who wants to get the most out of linux, though with the current crop of distributions like Mint and Ubuntu, it isn't strictly necessary anymore. There is a lot of debate in the Linux world over whether or not this is a good thing.

Linux Project #2 - Installing and Uninstalling Software on Debian Based Distributions
Under Windows, most people are used to going to the store, purchasing software, then coming home to install it with a CD or DVD. Linux works very differently when it comes to software. Because most software you will encounter in Linux is free or open source, you can simply take your pick of software and download it for free. There is no need to purchase or pirate bloated and restrictive proprietary software - everything you need is available for free.

Linux Project #3 - Sed and wget
Since we're using the command line, let's take a look at some really useful command line tools which we'll use again in a later project.

Linux Project #4 - Bash Scripting

Once you know what you want to do on the command line, it's not that much more work to make it into a script. A script is series of commands that you could have entered individually on the command line, but instead saved in a single file that can be run like a program. Learning how to do this to automate basic tasks is a huge time saver.

Linux Project #5 - Adding a Bash Script to Cron
If you have a script that you want to run on a regular basis, the best thing to do is to make it a "Cron Job" - Linux lingo that means add it to a list of scripts that run regularly with instructions on when to run it.
Linux Project #1 - Basic Bash Commands
Fire up your Linux terminal emulator of choice and let's get started! If you're running Mint or Ubuntu you can do this in several ways - look through your applications menu until you see something called "Terminal." Alternatively you can just type Alt+f2 then enter "gnome-terminal"
below we'll look at some of the basic commands you'll need to know to become a command line guru.


pwd
When you have your terminal emulator running, all you'll see is a prompt that probably looks something like:
username@computer ~ $
If you hit enter, it'll just drop to a new line and say the same thing again. What you're looking at is an interface to issue commands to your computer directly. The first command you'll learn is "pwd" which means "print working directory." When you type pwd at this prompt, it will return the directory you're currently in.
username@computer ~ $ pwd
/home/username
username@computer ~ $

ls
You can't do much just sitting in a directory, so we're going to issue another command: "ls". This command lists the directory contents.
username@computer ~ $ ls
Desktop Documents Downloads Music Pictures Public Templates Videos
username@computer ~ $
your results may vary depending on what you've got in your home directory.

mkdir
We're going to create a directory to work in for the rest of this project. Run the command "mkdir techprojects"
username@computer ~ $ mkdir techprojects
username@computer ~ $
It looks like nothing happened, but when you run "ls" again, you see what changed.
username@computer ~ $ ls
Desktop Documents Downloads Music Pictures Public techprojects Templates Videos
username@computer ~ $
we now have a directory to work in. We're left with a problem though: we're not in the right directory.

cd
The "cd" command allows you to change directories. We're going to cd into the techprojects directory we just created.
username@computer ~ $ cd techprojects
username@computer ~/techprojects $
My prompt changed when I did that (see the "/techprojects" between the "~" and "$"?). Yours might not change, but if you want to check where you are, you can always run "pwd" again. 
username@computer ~/techprojects $ pwd
/home/username/techprojects
username@computer ~/techprojects $
Let's change back up to the directory we were just in for a moment. There are a few ways to do this, so we'll talk about them one by one. Since we're going up one directory we can type "cd .." 
username@computer ~/techprojects $ cd ..
username@computer ~ $ pwd
/home/username
username@computer ~ $
the first time we switched to the techprojects directory, we were able to get away with just typing the name of the directory and not its full location, because it was in our working directory. You can't always do that. Go ahead and cd to the next highest directory by running "cd .." again, then try to cd into the techprojects directory by running "cd techprojects"
username@computer ~ $ cd ..
username@computer /home $ pwd
/home
username@computer /home $ cd techprojects
bash: cd: techprojects: No such file or directory
username@computer /home $
we can't use the same shortcut since the techprojects directory isn't in our working directory any more. We have to specify what's called an "absolute path" to the directory we want. We want to go to "/home/username/techprojects" so that's what we'll tell cd:
username@computer /home $ cd /home/username/techprojects
username@computer ~/techprojects $ pwd
/home/username/techprojects
username@computer ~/techprojects $

man
The user manual for linux is built right in. If at any time you forget what a command does, you can look up the manual for by typing "man" followed by the command you're looking for more information about. Try running "man ls" or "man pwd". When you're done looking at the manual, type "q" to quit.

clear
We've been running lots of commands, and our terminal window has a long history of everything we've just done. Sometimes this can be distracting, so you can get rid of all the old commands still in the terminal window by typing "clear"

date
There are some pretty useful functions built in to Linux. One is "date" which will print the current date and time. If you spend a few minutes reading the man page for date, you'll find that you can print more specific parts of the date by adding extra arguments. Try "date +%F" or "date +%r" as some examples.

cal
While we're on the subject of the date and time I should mention that Linux has a built in calendar. Too see it type "cal"
echo
You can make the terminal say anything you want by typing "echo" followed by whatever you want it to say.
username@computer ~/techprojects $ echo "hello world"
hello world
username@computer ~/techprojects $
variables
Just like in algebra, sometimes we need to use variables on the command line. You can assign variables just by using the equals sign like so: "x=5". make sure you dont leave any space between your variable, the equals sign, and the value. You can combine variables with echo to print the value of any assigned variable.
username@computer ~/techprojects $ x=5
username@computer ~/techprojects $ echo $x
5
username@computer ~/techprojects $
Note that when calling a variable (unlike assigning it) you must preface it with "$". If you'd like to, you can make the output of a command a variable.
username@computer ~/techprojects $ today=$(date +%F)
username@computer ~/techprojects $ echo $today
2012-03-04
username@computer ~/techprojects $
When I assigned the output of the date command to the variable "today" in the above command I had to format it appropriately so that Bash would know that I mean the output of the command rather than the words themselves. You can either do it the way I did it above, or enclose the command in backticks the key to the left of "1"
username@computer ~/techprojects $ now=`date +%r`
username@computer ~/techprojects $ echo $now
11:02:38 AM
username@computer ~/techprojects $

Redirecting Output with ">", ">>", and "|"
If you want to take the output of a command and save it to a file, you can just type "[command] > filename"
username@computer ~/techprojects $ date > today.txt
username@computer ~/techprojects $ ls
today.txt
username@computer ~/techprojects $
Using ">" will overwrite the contents of any file you redirect the output to, so be careful. You can use ">>" to add to the end of a file that already exists rather than overwriting it.
username@computer ~/techprojects $ echo "Fred" > bedrock.txt
username@computer ~/techprojects $ echo "Wilma" >> bedrock.txt
username@computer ~/techprojects $ echo "Dino" >> bedrock.txt
username@computer ~/techprojects $
If you need to send the output of one command or program into another command or program instead of a file, you can use a pipe "|" (shares a key with "\"). We'll come back to this later.

cat
cat prints the contents of a file. We can use it to check that what we just did actually worked.
username@computer ~/techprojects $ cat today.txt
Sun Mar  4 11:15:47 EST 2012
username@computer ~/techprojects $ cat bedrock.txt
Fred
Wilma
Dino
username@computer ~/techprojects $

grep
grep searches for a user specified string in a user specified location like a file or the output of a command
username@computer ~/techprojects $ grep Betty bedrock.txt
username@computer ~/techprojects $ 
This command returns nothing because "Betty" is not in the file bedrock.txt
username@computer ~/techprojects $ grep Wilma bedrock.txt
Wilma
username@computer ~/techprojects $ 
This command returns "Wilma" because "Wilma" is in the file bedrock.txt. grep is also used very frequently in conjunction with pipes. we can pipe the output of the command date to Grep to search it for a specific string.
username@computer ~/techprojects $ date | grep Sun
Sun Mar  4 11:43:03 EST 2012
username@computer ~/techprojects $ 
"Sun" is bold, and in my terminal it is also red. Grep then prints the rest of the line where it found the search term. You might think that this isn't very useful since it's very easy to find the search string manually with the output of the date command. This is true, but when looking for something within much larger output it can be very useful. look at the amount of output of the command: "ps -e". This is a list of every process currently running on your machine. You could run "ps -e | grep firefox" to see if firefox is running. 
cp
cp copies a file.

username@computer ~/techprojects $ cp bedrock.txt fredshouse.txt
username@computer ~/techprojects $ ls
bedrock.txt  fredshouse.txt  today.txt
username@computer ~/techprojects $ 

rm
rm removes (deletes) a file.

username@computer ~/techprojects $ rm bedrock.txt
username@computer ~/techprojects $ ls
fredshouse.txt  today.txt
username@computer ~/techprojects $ 

mv
mv moves a file to a new location or it changes its name. It's the same as using cp to create a new file then rm to remove the old one.
username@computer ~/techprojects $ mv today.txt yesterday.txt
username@computer ~/techprojects $ ls
fredshouse.txt  yesterday.txt
username@computer ~/techprojects $ 

exit
when you're ready to close your terminal you can type "exit"
Linux Project #2 - Installing and uninstalling software
in order to install software you need root access to the machine you're working on. If you're the one who installed your operating system, and you're running Mint or Ubuntu, all you need is your regular account password. If you're using Debian, Fedora, Slackware, or some other distro the process is slightly more complicated, and I'm not going to get into it here. If you're in this situation google "sudoers file."
If you're unfamiliar with the way linux manages software packages in repositories, or your distros particular package management system, you need to do some reading
I'm using a Debian based Distro that uses apt-get, so that's what we're going to use here. to install a package with apt-get when you know the name of the package, all you have to do is type "sudo apt-get install [package name]". let's do that with a package that we'll need for a later project.
sudo apt-get install wget
after you type the above in a terminal, you should see lots of text scroll down the screen. When it stops, your program is installed.
Linux Project #3 - Sed and Wget
This project uses two popular and powerful Linux programs, sed and wget. Wget downloads files (and much more) from the internet, and provides lots of options for how to get them. Sed (stream editor) looks at text passed to it and modifies it based on user spefified rules. It's a good idea to read the man pages for both programs before starting.
wget
let's start with a very basic use for wget: downloading a single file. You can download the html page you're looking at right now by running:
wget http://coldfusion.allegany.edu/smith191/grimsley/linux/03-tools.htm
as you can see, all that is required is the location of a file passed as an argument and wget will download the specified file to the working directory. Alternatively, if you had several files you wanted to download, you could add them to the file wgeturls.txt then type:
wget -i wgeturls.txt
There are a lot of other cools things you can do with wget, like the -k and -r arguments (read the man page), but we're going to use it in a pretty simple way today, so we don't need to get into all that. On to sed! 
sed
sed is an old program (it was written in the early 70's for Unix) but it's still one of the most useful utilities you can find in Linux and other Unix-like operating systems. Sed can do some pretty cool stuff, like replace every instance of a certain word in a file:
username@computer ~/techprojects $ echo "You want a toe? I can get you a toe." > dude.txt
username@computer ~/techprojects $ cat dude.txt
You want a toe? I can get you a toe.
username@computer ~/techprojects $ sed -e 's/toe/cat/g' dude.txt > reddit.txt
username@computer ~/techprojects $ cat reddit.txt
You want a cat? I can get you a cat.
You can also print everything between two words:
username@computer ~/techprojects $ echo -e "Dude \nMaude \nWalter \nDonny \nStranger" > big.txt
username@computer ~/techprojects $ cat big.txt
Dude 
Maude 
Walter 
Donny 
Stranger
username@computer ~/techprojects $ sed -n '/Maude/,/Donny/p' big.txt
Maude 
Walter 
Donny
username@computer ~/techprojects $
The Project
What I'd like to do is combine wget and sed to download the opinion page from my local newspaper, strip away all the excess code from the HTML file, and print a list of urls that link to the current letters to the editor hosted on the newspaper's website. The first thing to do is download the opinion page html file with wget. cd to a directory you'd like to work in and then run this command:
wget http://times-news.com/opinion/index.html
If you cat that file, you'll see that it's mostly excess html sitting around very few lines that we actually want. What we need to do is find something in the file that indicates the beginning and end of the section we're interested in, and luckily this is easy to do with html. The links we want are in a div called "headline_list" so we're going to use sed to print just the lines between where that div starts, and its end tag. The div starts with '<dl id="headline_list">' and ends with '</dl>'. Since "/" is a delimiter, we'll have to escape it with "\"
sed -n '/<dl id="headline_list">/,/<\/dl>/p' index.html > $(date +%F).opinionsection.html
This will create a new file named [yyyy-mm-dd].opinionsection.html which contains only the part of the file we are interested in, but it's still a lot of HTML, and it's way more than just links to articles. We'll use grep to find just the links. We do this by grepping for "<a href=" in the file we created in the last command, which will print all lines that start with that tag, then we redirect the output to a new file.
grep '<a href=' $(date +%F).opinionsection.html > $(date +%F).opinionlinks.html
Next we'll get rid of whatever html is still embedded in the links and leave behind the URLs only
sed -i -r 's/.*<a href=\"(.*)\".*/\1/' $(date +%F).opinionlinks.html
because of the way this newspaper formats the opinion page the first url repeats itself, so we should remove that line. We'll take care of this by removing any duplicate lines with "uniq"
uniq $(date +%F).opinionlinks.html > $(date +%F).opinionlinks.temp.html
mv $(date +%F).opinionlinks.temp.html $(date +%F).opinionlinks.html
There's a similar problem with the last line (it points to a second page with more headlines, which we're not interested in at all) so we'll take that off too.
sed -i -e '$d' $(date +%F).opinionlinks.html
finally we'll clean up a little bit, and since the file we were working on no longer contains links or any html, we'll rename it to something appropriate. 
rm index.html
rm $(date +%F).opinionsection.html
mv $(date +%F).opinionlinks.html $(date +%F).opinionurls.txt
We're left with just one file named [yyyy-mm-dd].opinionurls.txt and it contains exactly what we were looking for.</project>
Linux Project #4 - Bash Scripting
Our last project was great, but it took a lot of effort, and if it's the kind of thing that you plan on doing daily, it's not worth doing unless it's automatic. That's what computers are for, right? Let's take all of the commands from the last project, and just throw them into a text file, one to a line. That file would look like this:

wget http://times-news.com/opinion/index.html
sed -n '/<dl id="headline_list">/,/<\/dl>/p' index.html > $(date +%F).opinionsection.html
grep '<a href=' $(date +%F).opinionsection.html > $(date +%F).opinionlinks.html
sed -i -r 's/.*<a href=\"(.*)\".*/\1/' $(date +%F).opinionlinks.html
uniq $(date +%F).opinionlinks.html > $(date +%F).opinionlinks.temp.html
mv $(date +%F).opinionlinks.temp.html $(date +%F).opinionlinks.html
sed -i -e '$d' $(date +%F).opinionlinks.html
rm index.html
rm $(date +%F).opinionsection.html
mv $(date +%F).opinionlinks.html $(date +%F).opinionurls.txt

This is actually almost a script. We need to do a few things though. First we need to add a line to the top that tells the computer how to run the script. We wrote this in bash, so we need to point to the bash binary. You can find it by typing "whereis bash" but I'll save you the time and tell you that if you're running linux, it's almost certainly in "/bin/bash". We'll add this to the top of the file after a shebang or hashbang (#!).

#!/bin/bash

One other thing we might need to add is a line that makes sure the script creates the files in the right directory. The first 6 lines in the modified script that follows are an "if condition". If there is a folder called "opinionget" in the home directory of the user running the script, it cds to it. If there isn't it makes one, then cds to it. Either way, everything else that happens is done in that directory.

#!/bin/bash

if [ -d "/home/$(whoami)/opinionget" ]; then
        cd /home/$(whoami)/opinionget
else
        mkdir /home/$(whoami)/opinionget
        cd /home/$(whoami)/opinionget
fi

wget http://times-news.com/opinion/index.html

sed -n '/<dl id="headline_list">/,/<\/dl>/p' index.html > $(date +%F).opinionsection.html

grep '<a href=' $(date +%F).opinionsection.html > $(date +%F).opinionlinks.html

sed -i -r 's/.*<a href=\"(.*)\".*/\1/' $(date +%F).opinionlinks.html

uniq $(date +%F).opinionlinks.html > $(date +%F).opinionlinks.temp.html

mv $(date +%F).opinionlinks.temp.html $(date +%F).opinionlinks.html

sed -i -e '$d' $(date +%F).opinionlinks.html

rm index.html

rm $(date +%F).opinionsection.html

mv $(date +%F).opinionlinks.html $(date +%F).opinionurls.txt

We should probably be good and include some comments so people know whats going on here

#!/bin/bash

#check for a directory called "opinionget"; create it if it doesn't exist, then cd to it
if [ -d "/home/$(whoami)/opinionget" ]; then
        cd /home/$(whoami)/opinionget
else
        mkdir /home/$(whoami)/opinionget
        cd /home/$(whoami)/opinionget
fi

#dl the times-news opinion page
wget http://times-news.com/opinion/index.html

#delete all but div id="headline_list"
sed -n '/<dl id="headline_list">/,/<\/dl>/p' index.html > $(date +%F).opinionsection.html

#isolate lines that contain "<a href=" i.e. links; save to new file
grep '<a href=' $(date +%F).opinionsection.html > $(date +%F).opinionlinks.html

#strip all html from urls recovered in last step
sed -i -r 's/.*<a href=\"(.*)\".*/\1/' $(date +%F).opinionlinks.html

#remove duplicate lines. remove last line.
uniq $(date +%F).opinionlinks.html > $(date +%F).opinionlinks.temp.html
mv $(date +%F).opinionlinks.temp.html $(date +%F).opinionlinks.html
sed -i -e '$d' $(date +%F).opinionlinks.html

#clean up
rm index.html
rm $(date +%F).opinionsection.html
mv $(date +%F).opinionlinks.html $(date +%F).opinionurls.txt

Our script is done, but in order to run it, you need to make it executable. Save the script as opinionget.sh then cd to the directory it's in. run the following command to make it executable:

chmod +x opinionget.sh

Run the script by typing

./opinionget.sh
Linux Project #5 - Adding a Bash Script to Cron
The last project created a script, but in order to get the daily opinion page urls, we would still have to run the script manually every day. While this is way better than running each command individually every day, it's still not quite what we're looking for. The last step is to make the script a "cron job" i.e. add it to a list of scripts to run regularly. If you haven't done so already, go ahead and read the man page for crontab. Once you have an idea of what crontab is, go ahead and run
"crontab -l"
if you have anything in your crontab already you'll see it here. You can also see that when adding scripts, it is done in the following format:
*    *    *    *    *   /path/to/script
T    T    T    T    T
|    |    |    |    |
|    |    |    |    |
|    |    |    |    `----- day of week (0 - 6) (0 is Sunday, or use names)
|    |    |    `---------- month (1 - 12)
|    |    `---------------- day of month (1 - 31)
|    `-------------------- hour (0 - 23)
`------------------------- min (0 - 59)

some examples:

Run at 11AM Monday - Friday:
0 11 * * 1-5 /some/script

Run at 5:30PM on the 15th of the month, every month:
30 17 15 * * /another/script

Run every day at 4AM:
0 4 * * * /yet/another/script

The script we wrote in the last project should be run daily, as the newspaper updates its page daily. We should put it somewhere like /home/$(whoami)/bin/ so that we know its important (and if you want to you can add that directory to your path). Let's edit crontab.
crontab -e
add the following to the end of the file:
0 4 * * * /home/[your-user-name]/bin/opinionget.sh
If later you want to remove it, run "crontab -e" again and delete or comment out the line you just added. That's it. Every day you can look in the folder "getopinion" in your home directory to see a list of URLs to opinion articles from the Cumberland Times News... Get ready to rage.
Later we'll take this data and turn it into something a little more useful.
LAMP Server Administration Projects

 

LAMP Project #1 - Install and Configure Apache Web Server
We have already taken care of the first element of our LAMP server when we installed Linux during our first series of projects. Now we'll take care of the second element by installing Apache 2

LAMP Project #2 - Install and configure Webmin
Managing a server is difficult if you've never done it, and working strictly on the command line, though it is a suitable end-goal, is difficult to manage for those just getting started. Webmin provides a web interface for performing many tasks that would otherwise require extensive knowledge of the command line.

LAMP Project #3 - Install and Configure PHPMyAdmin
As in the last step, we can save ourselves a lot of command line hacking by installing a suitable web interface. In this instance, we're tackling the "M" in LAMP server - MySQL. Learning MySQL commands can be very difficult for beginners, so we're going to use PHPmyAdmin, which is much more intuitive, and easier to learn, which will allow you to spend more time coding and less time kicking yourself for accidentally issuing DROP TABLE incorrectly. To be clear: PHPMyAdmin won't keep you from screwing up MySQL, but it does make it easier not to. 

LAMP Project #4 - Python
The last element of our LAMP server is Python/PHP (no insult to Perl intended! maybe some other time). We'll break it into two parts. In this first part, we'll write a python script that adds data generated by the bash script in the Linux section to a table in the MySQL database we created in LAMP project #3

LAMP Project #5 - PHP
The second half of the previous project will use PHP to read the data that we just entered into our databse. If time permits, we'll lament the lack of a working PHP installation on the server that hosts ACM web development projects
LAMP Server Administration Project #1 - Install and Configure Apache Web Server
In order to install software, again, you'll need root access. To install Apache in Debian based Distros, type:
sudo apt-get install apache2
This might take a while, but when it's done, you should be good to go with apache. We'll have to do a little tweaking, but for the most part it's already up and running. I'm going to do the rest of this tutorial assuming you're running Mint or Ubuntu or another very similar distro, so if you're running something else, be sure to check your documentation (really, you should be doing this anyway no matter what distro you're running). The first thing we can do is check to see if the server is running. run:
ps -e | grep apache
to see if you have a process running already. If not, try to start it with
sudo service apache2 start
When it's running, point your web browser to 127.0.0.1 and see what happens. If everything went well you should see a simple page that says "it works!". This file is "/var/www/index.html" by default. You can edit the file, or delete it and replace it with whatever you want. Anything you have in the directory /var/www/ (called "web root") will be accessible by typing your computer's IP address into the address bar in a web browser - of course unless you have changed your router's settings, it will only work on your own LAN, and everything is subject to the file permissions you set - if none of the files in the web root have read permissions you'll get an error 403 in a browser.
If Apache is working, we'll need to make a few changes so that things we will do in later tutorials will work. Feel free to skip this and come back to it later if you run into an error condition or something, and as always before you do anything you should consult the documentation. The first thing we need to do is enable mod rewrite
sudo a2enmod rewrite
You may also need to change the line "AllowOverride None" to "AllowOverride All" under "Directory /var/www/" in the file /etc/apache2/sites-available/default" 
LAMP Server Administration Project #2 - Install and configure Webmin
Up until now we've spent a lot of time on the command line, but sometimes it's nice to have a GUI. Webmin provides a sleek, intuitive web interface for completing basic and advanced administration tasks. If you think you'd rather do things in hard-core mode, feel free to skip this project. You can get webmin at www.webmin.com. It's not in the repositories for Ubuntu or Linux Mint AFAIK, but the webmin development team has gone through the trouble of posting a .deb for us, so we won't have to go too far out of the way to get this done. Go ahead and download that deb, then cd to the directory it's in and run
sudo dpkg -i webmin.version.whatever.deb
of course you're going to want to substitute the correct filename. It'll go through an install process, and when it's done it'll spit out some information about how to access the web interface. Usually it's "your-ip:10000" or "localhost:10000" or "127.0.0.1:10000" or something like that. You'll be presented with a login screen - login and you'll be presented with an information page that shows you a little bit about the maching you're running. Be careful about what you do here - keep in mind, this is a UI for system administration, so you shouldn't modify anything unless you know what you're doing. Feel free to look around though, you won't mess anything up unless you cange it first.
Click on "Servers" in the menu on the left. It should expand to show that you're running Apache Webserver. Click on that to see settings associated with your Apache install.
If you expand the system menu, then click "scheduled cron jobs" you can see what's going on in cron. Also in the system menu you can click "running processes" to see a list of every process currently running. This is the same as typing "ps -e" in a terminal window, but the output is much easier to look through with this UI. Webmin is capable of doing most system administration tasks, and you can consult the webmin docs for a full list of what and how. A later project will involve running a squid proxy, and having webmin installed will help out a lot.
LAMP Server Administration Project #3 - Install and Configure PHPMyAdmin
The "M" in LAMP stands for MySQL, which is a database program that runs as a server. If you plan on managing a lot of data: by generating it, storing it, manipulating it, or whatever else; it makes sense to use a database. I'm running Ubuntu server, which has MySQL installed by default, but if you are using something else, you may need to install it.
sudo apt-get install mysql-server

Installing PHPMyAdmin
Managing MySQL takes a lot of knowledge because the syntax takes some getting used to. Over the command line, it can be very difficult for a beginner to wrap their head around what's actually happening when you issue commands. That's why it can be helpful to have a Web interface. Just like Webmin, PHPMyAdmin allows a system administrator to make changes to the settings of an otherwise difficult to understand CLI program via a GUI. To be clear: I think everyone should learn to do things over the command line, but I also think that starting out that way entirely can be prohibitively intimidating. PHPMyAdmin can be installed with apt-get
sudo apt-get install phpmyadmin
In this project we're going to create a database to manage all the data that will be generated with the bash script written in linux project #4. Keep this in mind as we continue.

Setting up a User and Database
Once PHPMyAdmin is installed, you can get to the web interface by pointing your browser to "127.0.0.1/phpmyadmin", or "your-local-ip/phpmyadmin". Once you're in, we need to create a new user to manage a database, as it's probably not best to run everything as root. Click on "privileges" then "add a new user". Choose whatever username you want, but keep in mind, later we'll be choosing an option that will simultaneously create a database with the same name, so it makes more sense to choose something that describes the database we'll create than it does to pick something like "joe". In other words, the user you're creating here is not a user that you run phpmyadmin with, it's the user that owns and maintains the database we need for this project. I chose the user name "opinionpage". Select "localhost" as the host, and generate a random password. Be sure to write down the password and save it somewhere safe. As far as I know (and I've been in this situation before) you can't recover this password, only reset it, so don't lose it. Make sure to check "create database with same name and grant all privileges", then create the user.

Setting up a Table
Once you have a new database, we need to set up a table within it. The table will hold data collected by our Bash script, and it won't have to be very complicated. We will only need three columns to hold the data - an auto-incrementing index, a date column, and a column to hold all the URLs generated by the script each day. If we had to visualize what this is going to look like, it would look something like this:
     |-----------------------------------------------------------|
     |                opinionpage (our database)                 |
     |-----------------------------------------------------------|
     |       index      |         date         |       urls      |
     |------------------|----------------------|-----------------|
     |        01        |      2012-03-01      | [URLs from 3/1] |
     |------------------|----------------------|-----------------|
     |        02        |      2012-03-02      | [URLs from 3/2] |
     |------------------|----------------------|-----------------|
     |        03        |      2012-03-03      | [URLs from 3/3] |
     |------------------|----------------------|-----------------|
     |        04        |      2012-03-04      | [URLs from 3/4] |
     |------------------|----------------------|-----------------|
     |        05        |      2012-03-05      | [URLs from 3/5] |
     |------------------|----------------------|-----------------|
     |        06        |      2012-03-06      | [URLs from 3/6] |
     |------------------|----------------------|-----------------|
     |        07        |      2012-03-07      | [URLs from 3/7] |
     |-----------------------------------------------------------|


Each column is a data category and each row is a new entry. It is all contained within the database called "opinionpage". here is the data you'll need to set up each of those columns. If I don't specify something, then just go with the default or leave it blank.
Column 1
column: index
type: int
index: primary
auto increment: checked

Column 2
column: date
type: date

Column 3
column: urls
type:text
Click "save" at the bottom. Our database and table is set up and ready to go. There is still nothing in it, so we'll do that with Python in the next project.
LAMP Server Administration Project #4 - Python
So far we have created a bash script that downloads an html page, then grabs urls from it, formats them and saves them to a file. Then we put that script in cron so it runs daily. Next we set up an apache web server, installed some basic web UI tools, and set up a MySQL database and table. If we're going to put those things to good use, we need more. In this project we'll write a Python script (and put it in cron) that adds the urls generated by our bash script to the table in our database.

Python
Python is a great language with a very simple syntax. It's the first language I ever tried to learn (though I later abandoned it [temporarily] in favor of PHP). It's easier to understand by looking at the code than some other languages, but it's still very powerful. This script will use the MySQLdb module which you might have to install
The first thing the script will need to do is get the date in the appropriate format, then add it to a string containing a path to the file generated by the bash script so we can point to the right file depending on what day it is. 
import datetime
today=str(datetime.date.today())
filename="/var/www/news/times-news/"+today+".opinionurls.txt"
You can't add strings and objects in python, and the date is an object, so it needs to be treated like a string. That's what "str()" is for. We then declare a variable called "filename" (which is a string) that contains the location of the text file with the urls in it. I put mine under web root in a folder hierarchy "news/times-news/". this means that to look at the file in a web browser, I'd go to 
"http://127.0.0.1/news/times-news/2012-03-05.opinionurls.txt"
The bash script from the previous project was designed to run as a regular user, so it dropped the files in the home directory of the user that ran it. If you want to drop the url-containing text files in a folder on your web server, you'll have to modify the bash script to reflect the change, and then run it as root (take it out of your cron tab, and put it in root's cron tab).
Now that python knows where the file is, we'll open it
f = open(filename, 'r')
urls=str(f.read())
In the code above, we open the file whose location is stored as the variable "filename" in read mode as the variable "f". We treat that like a string with the variable "urls". Next we need to connect to the database
import MySQLdb
conn = MySQLdb.connect(host="localhost",user="opinionpage",passwd="YOUR-PASSWORD-HERE",db="opinionpage")
cursor = conn.cursor()
The previous example imports the necessary module, connects to the specified database with the supplied credentials and gets ready to input data
cursor.execute("""INSERT INTO opinionurls VALUES(NULL,CURDATE(),%s)""", (urls))
conn.commit()
Notice the string substitution above. If you attempt to put a variable directly into cursor.execute, it will either give you an error or add the string of the variable name to the table depending on if you use quotes or not.
We can put it all together into a script
#!/usr/bin/python

import MySQLdb
import datetime

#get the date, use it to create a variable with a link to our file
today=str(datetime.date.today())
filename="/var/www/news/times-news/"+today+".opinionurls.txt"

#open the file
f = open(filename, 'r')
urls=str(f.read())

#connect to the database
conn = MySQLdb.connect (host = "localhost",user = "opinionpage",passwd = "YOUR-PASSWORD-HERE",db = "opinionpage")
cursor = conn.cursor()

#add data from file to table
cursor.execute("""INSERT INTO opinionurls VALUES(NULL,CURDATE(),%s)""", (urls))
conn.commit()
Add this script to cron. It uses data from the bash script also in cron, so it's important that the python script runs sometime after the bash script is done. On my server the bash script only takes a few seconds, so it's fine to add the bash script at 5:00AM and the python script at 5:01AM. At this point we have a functional database with new data added to a table on a daily basis, but we still can't do anything with it. In the next section we'll make this data work for us with PHP.
LAMP Server Administration Project #5 - PHP
PHP provides a quick and easy way of viewing the data we just added to the table. All you have to do to run a PHP script is put a file ending in .php on a webserver and navigate to it in a web browser. Make sure PHP is installed before you continue. You can type "php --version" in a terminal to see if it's there, and if not, apt-get it. The script we'll create here will be relatively simple. Keep in mind that this is just a starting point - If you want to really make this shine, you'll have to add some html and css, and even extend the use of php a little bit.
The first thing to do in our PHP script is declare a bunch of variables which we'll use later to connect to the database.
<?php

$username="opinionpage";
$password="YOUR-PASSWORD-HERE";
$database="opinionpage";
$date=date('Y-m-d');

?>
Next we'll connect to the database
<?php

mysql_connect('localhost',$username,$password)
    or die('Could not connect: ' . mysql_error());
mysql_select_db($database) or die('Could not select database');

?>
The above is straight out of the PHP Manual so I'm not going to get into it.
<?php

$current=mysql_query('SELECT * FROM opinionpage WHERE date = "'.$date.'"');
$row = mysql_fetch_array($current);
$urlsarray = explode("\n", $row['urls']);

?>
Let's break this down line by line. 
Line 1: Create the variable $current from the output of a mysql query (the part in parentheses). The MySql query asks for everything (*) in the row from the opinionpage table (opinionpage) where the date column (date) is the same as the evaluated output of the variable $date, which is equal to "date('Y-m-d')", which prints the current date as yyyy-mm-dd. In other words, it asks for the row of data created today. 

Line 2: Create the variable $row which is an array created from the row of data selected in the previous line. We'll be able to manipulate this array as any other, but it is associative, using the column names as keys. We can, for example, ask for the data in the date column with $row['date'] or the index column with the variable $row['index']. 

The last line of the above block uses the $row array created in the second line. When we call $row['urls'] we get the same thing as the entire text file created with the bash script. Having the data presented in this way isn't particularly useful, so we need to be able to grab distinct values from it somehow. To solve this problem, we create a new array $urlsarray by using explode to separate elements in $row['urls'] based on where a new line occurs, such that every individual line of text originally in the text file we created with bash (which was then moved into MySQL) will become a distinct value in the new array. Because each url occupies its own line, we now have an array where each url is a different value. We can call the first url with $urlsarray[0] or the second with $urlsarray[1] and so on. If we wanted to know how many elements are in the array, we could use $count(), though because of the way we'll use this data, it doesn't matter how many elements there are. We'll use a foreach loop, so however many urls there are is how many times we'll work with them.
<?php

foreach ($urlsarray as $url){
        
        $locs = str_replace("http://times-news.com/opinion", "", $url);
        $pattern='#(.*)/x(.*?)/#is';
        $replacement="";
        $titled=preg_replace($pattern, $replacement, $locs);
        $title = str_replace("-", " ", $titled);
        echo "<a href='".$url."'>".$title."</a><br>";
        }

?>
we then run a foreach loop, doing some string operations on each item in the array (removing everything but the filename from the url, then removing the dashes from between each word in the filename and replacing them with spaces.) Then we take both the cleaned up filename, and the raw url, and format them appropriately as a link in the form: "<a href='url'>title</a><br>". This same operation is done on every url in the row. Again, let's look at this line by line.
foreach ($urlsarray as $url){
foreach will go through the array $urlsarray and process each element as its own variable: $url. everything that happens in the loop happens once for each element. Instead of writing code that brings up element 1, does something to it, saves it as a new variable and moves on to element 2, foreach will automate that, making less work and way less code.
$locs = str_replace("http://times-news.com/opinion", "", $url);
This line looks for the text "http://times-news.com/opinion" and then replaces it with nothing. If you wanted to, you could use str_replace to substitute some other text, but we just want to get rid of it, so after the comma we have quotes with nothing in them, then the object we want to perform the operation on: in this case $url, which is the current iteration in the for each loop.
$pattern='#(.*)/x(.*?)/#is';
$replacement="";
$titled=preg_replace($pattern, $replacement, $locs);
$pattern establishes a regular expression that will be used by preg_replace. $replacement is what the text that matches the regular expression will be replaced with, in this case nothing. Finally we put that into preg_replace. 
$title = str_replace("-", " ", $titled);
The $title variable takes everything that was done before (which has left us with something like: "this-is-the-letter-title") and removes the dashes and replaces them with spaces.
echo "<a href='".$url."'>".$title."</a><br>";
finally we echo both the full url as well as the cleaned up title within html link tags so that we have a link to the letter in question with the correct title. Because of the foreach loop this is done on every url in the array.
All that's left is to package this up in a nice .php file, throw it on the webserver, and we're done.
<html>
<head>
<title>
Opinion RAGE!!!!!
</title>
</head>
<body>

<?php

//declare variables for connecting to db
$username="opinionpage";
$password="YOUR-PASSWORD-HERE";
$database="opinionpage";
$date=date('Y-m-d');


//connect to db
mysql_connect('localhost',$username,$password)
    or die('Could not connect: ' . mysql_error());
mysql_select_db($database) or die('Could not select database');


//get row from db for today
//create array from row
//explode urls column into its own array
$current=mysql_query('SELECT * FROM opinionpage WHERE date = "'.$date.'"');
$row = mysql_fetch_array($current);
$urlsarray = explode("\n", $row['urls']);


//format and print the urls
foreach ($urlsarray as $url){
        
        $locs = str_replace("http://times-news.com/opinion", "", $url);
        $pattern='#(.*)/x(.*?)/#is';
        $replacement="";
        $titled=preg_replace($pattern, $replacement, $locs);
        $title = str_replace("-", " ", $titled);
        echo "<a href='".$url."'>".$title."</a><br>";
        }


?>

</body>
</html>
Every day when you look at that same page, it will change automatically to display a list of links that reflects the current letters to the editor as posted on the Cumberland Times-News webpage. This setup leaves open the possibility of looking at links for previous day's letters as well, all you'd have to do is add an html form that specifies the date and uses that user input instead of the current date. Since the urls are being added to a database automatically each day, the data is always there at your disposal. It's up to you to figure out what you want to do with it.
Network Projects

 

Network Project #1 - Installing Custom Router Firmware
You probably have a router connected to your modem that you use to share your internet connection with multiple computers. Most store-bought routers are capable of a lot more than they deliver - many companies build several versions of their firmware that they put on more or less identical hardware, only to sell the more "advanced" models for far more money. We've learned about the importance of running free software in the Linux project section, but now we'll discover that we can run the firmware of our choice on many routers, vastly increasing the functionality while at the same time spending no money.

Network Project #2 - Re-purpose Old webcams as Live-Streaming Security Cameras
If you're like me, you've got a box of old hardware sitting around somewhere. I can't bring myself to throw out old keyboards, cables, webcams, or whatever else, so I've got a spare parts box in my basement that I make frequent use of when completing new projects. One way to easily use some of that old hardware is to attach an old webcam to a network connected computer, and stream the output over your LAN. I have a webcam mounted just outside of my office window pointed at my front door. When someone comes to the door, I can check to see who it is before going to answer it. This is Tony Montana level security for almost no cost.

Network Project #3 - Use Squid Proxy to Filter Out Unwanted Content from your Network
If you've been paying close attention to the way the Internet has been changing recently, you may have found yourself upset over the extent to which targeted ads and behavior tracking have invaded seemingly innocuous content. Nearly every page you visit has a facebook like button, and believe it or not, the like button exists (at least in part) as a method of tracking you across the internet. Hundreds of companies have suddenly cropped up to serve the rapidly expanding market of targeted advertising. If you are interested, you can see who is tracking you by installing a firefox addon called Collusion. I don't care if it's for advertising or any other reason, I don't want to be tracked, so I run all of my internet traffic through a filter that removes known trackers, servers that host nothing but ads, phishing sites, and other undesireable web content. This project will explain how to install and configure Squid web proxy.
Network Project #1 - Installing Custom Router Firmware
You probably didn't know that you could swap out the firmware on your router to unlock its true potential. I'm sure the router manufacturers like it that way - you'll have to spend more to get more functionality. Of course it doesn't have to be that way.
On Hacking
Hardware is hardware, and if you own it, you should have the right to make it do whatever you want. This is what hacking is all about. We've seen this recently with the spread of smart phones that are essentially locked down into a state of reduced functionality, forcing users to install only proprietary software distributed only through channels approved by the manufacturer (and subject to it's censorship). Needless to say, this is not the best model for free use of software, or for free speech, or even for accomplishing every day tasks with the technology in question. What it is, is a business model - and it has worked well for those on top. I'd like to use my hardware to accomplish my own goals rather than for the benefit of whoever is selling it. When I see that something I own has more potential - I'll modify it so that it lives up to that potential. When most people hear the word "hacking" they think of identity thieves and people who write viruses, but I (and many others) think that hacking means taking control of your own hardware (or more broadly - of your own life) by asking questions about how things work, by making changes to things that you think aren't working the way you want them to
Hacking your Router with Custom Firmware
There are several current projects offering custom firmware for routers: dd-wrt, Tomato, and OpenWrt. You'll want to look at each page to see what hardware is supported by each project, then choose a project based on the features you're looking for. Because the range of hardware available is so wide, and the versions of the custom firmware varies so much, I'm not going to spend time with a walkthrough on how to install this firmware on any particular type of hardware (especially since it's already been done by each of these projects themselves), but I would like to give you a sense of the benefits of installing custom firmware.
The first reason is that when you're running free or open source firmware, you have control over it. When it comes to routing, you want as much control as you can get. I recently had the opportunity to look at the Actiontec GT701C and I was shocked to find that not only does the default firmware log every website any networked computer looks at, but it also provides the owner of the hardware with no method of deleting the log, or stopping it from tracking your activity. Why would anyone want that? Well, I can think of some reasons why some people might want it, but not the end users - this heavy handed method of taking control from the owner of the hardware does not benefit the consumer, it benefits the manufacturer, and possibly the ISP or the government, who both maintain logs on internet activity and may have an interest in decentralized logging as a backup to the ones they already keep. If you're running this hardware and would like to learn how to disable the logs, check out this blog post
Another reason you may want to run custom router firmware is the fine grain control over your routing that it provides. Most of these projects allow users to directly enter iptables rules, but they also provide a user interface for configuring things more easily. One example of something that you can do is create a vlan, then write rules to keep that vlan completely separate from the rest of your network, so you could have a wireless connection for your family that gives access to local resources, and one for guests that only allows access to the internet. This kind of control is worth having if you frequently get into technology related projects.
Network Project #2 - Re-purpose Old webcams as Live-Streaming Security Cameras

This project will use mjpgstreamer and zone minder in conjunction with an existing webserver in order to stream output of webcams to a local webpage.
The first step is to get mjpgstreamer
wget http://sourceforge.net/projects/mjpg-streamer/files/mjpg-streamer_r94-1_i386.deb
because this is an older project, and the debian rules for version numbers are being enforced differently, you can't just install that package with "dpkg -i" - you'll have to repackage it first. Debian used to allow alphanumeric version numbers like this package contains, but now if you try to install it, you'll get an error, so we have to change the version number before installing the package. Extract the filesystem tree to a directory "packagename.x.x.x":
dpkg-deb -x packagename.x.x.x.deb packagename.x.x.x
Extract the control file information and cd into the directory:
dpkg-deb -e packagename.x.x.x.deb packagename.x.x.x/DEBIAN
cd packagename.x.x.x/DEBIAN
Edit the file called "control" and change the desired version number for the dependency that you want to change. Once that's done, rebuild the deb package:
dpkg-deb -b packagename.x.x.x packagename.x.x.x.deb
now use "dpkg -i" to install the modified package. When it's installed you can start your camera with mjpg streamer. You'll need to know where in /dev/ your camera is. It's probably /dev/video0 or /dev/video1, but you should unplug the camera, plug it back in, then run dmesg to see where it gets assigned so you know for sure. You can start your camera stream by running something like the following:
sudo mjpg_streamer -i "input_uvc.so -d /dev/video0/" -o "output_http.so -w /var/www -p 8099"
Where /dev/video0/ is changed to the appropriate address for your camera, and -p 8099 is set to whatever port you want to stream to. Now navigate your browser to "http://127.0.0.1:8099/?action=stream" to see the raw video stream.
If you want to take this to the next level, install Zone Minder and add your network stream as a camera in Zone Minder.
Network Project #3 - Use Squid Proxy to Filter Out Unwanted Content from your Network
Squid is a caching proxy which can be used to speed up your browsing, or filter out sites, IPs, or even certain types of content based on rules called access control lists (ACLs). Start by apt-getting squid. We'll have to edit the configuration file, but before we do, we should back it up. The conf file is "/etc/squid/squid.conf"
Sudo cp /etc/squid/squid.conf /etc/squid/squid.conf.bak
sudo gedit /etc/squid/squid.conf
By default all traffic is denied ("http_access deny all").edit the file to add your local network by adding these lines
acl intranet 192.168.1.0/24
http_access allow intranet
Make sure that the line "http_access allow intranet" is above the line that says "http_access deny all". You can change the port that squid runs on by changing "http_port" but it isn't necessary. The default is 3128.
When you start squid for the first time, you need to create the swap directories. Do that with the -z argument.
sudo squid -z
When that's done, start squid normally
sudo service squid start
To configure your browser to use squid, you'll need to change its network settings. In firefox that's in preferences -> Network -> settings. Change http, https, and ftp proxy to the ip address of your squid server on port 3128. If you make this change in your browser, and everything still works - you're using squid successfully.
The next step is to create some ACLs to filter unwanted websites from our network. Run
sudo nano /etc/squid/block.acl
to open a text editor to create a file that will contain a list of blocked sites. Enter into this file the following:
.facebook.com
.facebook.net
.fbcdn.net
Save the file, then open squid.conf again. In the acl section add the following
acl blocksites dstdom_regex "/etc/squid/block.acl"
Now scroll down in the squid.conf file, and above "http_access allow intranet" at the very top of the list of "http_access" rules, add the following line:
http_access deny blocksites
Every site in the list "/etc/squid/block.acl" will be blocked after restarting squid
sudo service squid restart
open the browser you configured to work with squid previously and try to open www.facebook.com. You should see a page saying that access is denied. There are lots of rules available in squid - you can block certain sites for certain computers at certain times of the day. You could redirect traffic from one site to another, you can stop users from looking at videos, or hundreds of other types of things. Take a look at the squid docs to learn more about it. If you installed webmin in LAMP Server Administration Project #2 then you can use webmin to manage squid. You don't have to open squid.conf if it's intimidating - go ahead and create your ACL's with webmin instead (to be clear, you should still learn to do it via the .conf file). To look at squid in webmin, select "Servers" from the left menu, then click "squid". If it isn't there you should be able to find it by searching for "squid" in the search box. 

